<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Google Cloud: AI Hypercomputer</title>
  <subtitle>Changelog for Google Cloud product: AI Hypercomputer</subtitle>
  <updated>2025-12-12T00:00:00+00:00</updated>
  <id>urn:github:multani:gcp-changelog:ai-hypercomputer</id>
  <link rel="self" href="https://raw.githubusercontent.com/multani/gcp-changelog/refs/heads/main/content/ai-hypercomputer/"/>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-12-12:issue:0</id>
    <published>2025-12-12T00:00:00+00:00</published>
    <updated>2025-12-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Workloads on A4 VMs might experience interruptions due to a firmware issue
for NVIDIA B200 GPUs. To help prevent the issue, we recommend resetting the GPUs
on A4 VMs at least once every 60 days. For more information, see the
<a href="https://docs.cloud.google.com/ai-hypercomputer/docs/troubleshooting/known-issues#a4-firmware">known issue</a>.</p>]]></content>
    <summary>A firmware issue with NVIDIA B200 GPUs may cause interruptions on A4 VMs. To mitigate this, reset your GPUs on A4 VMs at least once every 60 days.</summary>
    <title>Issue: GPU Firmware Issue on A4 VMs</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-12-09:feature:0</id>
    <published>2025-12-09T00:00:00+00:00</published>
    <updated>2025-12-09T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can enable node health prediction in an
AI-optimized GKE cluster to help the cluster avoid scheduling workloads on nodes
that are likely to degrade within the next five hours. This approach helps
minimize interruptions for critical and interruption-sensitive workloads, such
as large-scale training. For more information, see
<a href="https://docs.cloud.google.com/ai-hypercomputer/docs/workloads/enable-node-health-prediction">Enable node health prediction in a GKE cluster</a>.</p>]]></content>
    <summary>AI-optimized GKE clusters now support node health prediction, a new feature that helps avoid scheduling workloads on nodes likely to degrade within five hours. This enhancement minimizes interruptions for critical and interruption-sensitive workloads like large-scale training.</summary>
    <title>Feature: Node health prediction for GKE</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-10-21:feature:0</id>
    <published>2025-10-21T00:00:00+00:00</published>
    <updated>2025-10-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can use future reservations in
AI Hypercomputer to request to reserve capacity starting on a specific
date up to one year in the future. For more information, see
<a href="https://docs.cloud.google.com/ai-hypercomputer/docs/reserve-capacity">Reserve capacity</a>.</p>]]></content>
    <summary>You can now reserve capacity in AI Hypercomputer starting on a specific date up to one year in advance. This feature allows for better planning and resource management.</summary>
    <title>Feature: Future reservations are now available</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-09-10:feature:0</id>
    <published>2025-09-10T00:00:00+00:00</published>
    <updated>2025-09-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: The <a href="https://cloud.google.com/ai-hypercomputer/docs/gpu#a4x">accelerator-optimized A4X machine type</a>, the first GPU VM to run on Arm, is available on AI Hypercomputer. The A4X machine series has the NVIDIA GB200 Grace Blackwell Superchips attached and runs on the NVIDIA GB200 NVL72 platform. Use this machine type to run your large artificial intelligence (AI) models and machine learning (ML) workloads. The A4X machine type is currently available in the <code>us-central1-a</code> zone.</p>]]></content>
    <summary>The new accelerator-optimized A4X machine type, featuring NVIDIA GB200 Grace Blackwell Superchips, is now generally available. This GPU VM, running on the NVIDIA GB200 NVL72 platform, is ideal for large AI and ML workloads and is currently available in the `us-central1-a` zone.</summary>
    <title>Feature: A4X machine type now generally available</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-09-10:feature:1</id>
    <published>2025-09-10T00:00:00+00:00</published>
    <updated>2025-09-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can receive a notification when maintenance for an A4X reservation sub-block is scheduled, starts, or is completed. Additionally, you can now view and trigger maintenance for an A4X reservation sub-block. These features give you more control over maintenance for your A4X reservations, helping you minimize downtimes for your workloads. For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/manage/host-events-reservations">Manage host events across reservations</a>.</p>]]></content>
    <summary>Stay informed and in control of your A4X reservation maintenance with new notifications and management features.</summary>
    <title>Feature: A4X reservation maintenance notifications and controls</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-09-10:feature:2</id>
    <published>2025-09-10T00:00:00+00:00</published>
    <updated>2025-09-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You can receive at least seven days of advance notice for unplanned hardware maintenance for a reservation. This feature helps you more proactively control disruptions to your workloads when unplanned maintenance is scheduled after a host error or faulty host report. For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/manage/host-events-reservations#emergency-notifications">Manage hardware emergency maintenance notifications</a>.</p>]]></content>
    <summary>Receive at least seven days of advance notice for unplanned hardware maintenance for a reservation.</summary>
    <title>Feature: Advance notice for hardware maintenance</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-09-10:feature:3</id>
    <published>2025-09-10T00:00:00+00:00</published>
    <updated>2025-09-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can use the following Cloud Monitoring metrics to monitor your A4X VMs, and help you identify and troubleshoot issues with your GPUs:</p>
<ul>
<li>NVLink runtime error</li>
<li>Uncorrectable DRAM ECC errors</li>
<li>Uncorrectable DRAM row remapping count</li>
<li>Uncorrectable DRAM row remapping failed</li>
<li>Uncorrectable PCIe errors</li>
<li>Uncorrectable cache ECC errors</li>
</ul>
<p>For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/monitor">Monitor VMs and Slurm clusters</a>.</p>]]></content>
    <summary>New metrics are available to monitor your A4X VMs and troubleshoot GPU issues.</summary>
    <title>Feature: New metrics for A4X VM monitoring</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-09-10:feature:4</id>
    <published>2025-09-10T00:00:00+00:00</published>
    <updated>2025-09-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can view and manage the topology of your A4X reservations, including sub-blocks. This feature helps you better understand the topology of the VMs in your workload to further minimize network latency, as well as understand the health of your reservation blocks or sub-blocks. For more information, see
<a href="https://cloud.google.com/ai-hypercomputer/docs/view-reserved-capacity">View reserved capacity</a>.</p>]]></content>
    <summary>You can now view and manage the topology of your A4X reservations, including sub-blocks. This helps you understand VM topology for reduced network latency and monitor reservation health.</summary>
    <title>Feature: View A4X reservation topology</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-09-10:feature:5</id>
    <published>2025-09-10T00:00:00+00:00</published>
    <updated>2025-09-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: When you reserve capacity for creating VMs, you can specify the reservation operational mode for your reserved capacity. A reservation operational mode defines how your VMs behave after a host error or faulty host report, and it determines your level of visibility and control over the reservation's infrastructure. For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/create/review-configurations#reservation-operational-mode">Reservation operational mode</a>.</p>]]></content>
    <summary>You can now specify the reservation operational mode for your VM capacity reservations. This setting determines how your VMs behave in case of host errors, offering different levels of control and visibility over the underlying infrastructure.</summary>
    <title>Feature: Reservation operational mode for VMs</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-09-10:feature:6</id>
    <published>2025-09-10T00:00:00+00:00</published>
    <updated>2025-09-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: When you reserve capacity for creating VMs, you can specify a maintenance scheduling type for your reservations. This feature helps you minimize downtimes by letting you specify whether you want to group VMs and have synchronized maintenance scheduling (<em>grouped</em>), or loosely couple VMs have independent maintenance scheduling (<em>independent</em>). For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/create/review-configurations#maintenance-scheduling-types">Maintenance scheduling types</a>.</p>]]></content>
    <summary>You can now specify maintenance scheduling types for your VM reservations to minimize downtime.</summary>
    <title>Feature: VMware Cloud Foundation on Google Cloud VMware Engine</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-07-18:feature:0</id>
    <published>2025-07-18T00:00:00+00:00</published>
    <updated>2025-07-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can troubleshoot workloads with slow performance by using straggler detection metrics and logs.</p>
<p><em>Stragglers</em> are single-point, non-crashing failures that eventually
slow down your entire workload. Large-scale ML workloads are very susceptible to stragglers, and VMs with stragglers are often very difficult to notice and pinpoint without straggler detection.</p>
<p>For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/monitor">Monitor VMs and Slurm clusters</a> and <a href="https://cloud.google.com/ai-hypercomputer/docs/troubleshooting/troubleshoot-slow-performance">Troubleshoot slow performance</a>.</p>]]></content>
    <summary>Troubleshoot slow performance in your workloads by using new straggler detection metrics and logs. This feature helps identify and resolve single-point, non-crashing failures that can slow down your entire workload, especially in large-scale ML workloads.</summary>
    <title>Feature: New Feature: Straggler Detection for Performance Troubleshooting</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-07-10:feature:0</id>
    <published>2025-07-10T00:00:00+00:00</published>
    <updated>2025-07-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can now manage the <a href="https://cloud.google.com/ai-hypercomputer/docs/nccl/comma">Collective Communication Analyzer (CoMMA)</a>, a library that uses the NVIDIA Collective Communication Library (NCCL) profiler plugin to collect detailed NCCL telemetry for GPU machine types. The collected performance metrics and operational events are used for analyzing and optimizing large-scale AI and ML training workloads.</p>
<p>CoMMA is automatically installed and enabled on A4X, A4 High, and A3 Ultra machine types when using specific images. You can manage this data collection by disabling the plugin, adjusting its data granularity levels, or manually installing it on other GPU machine types. For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/nccl/configure-comma">Enable, disable, and configure CoMMA</a>.</p>]]></content>
    <summary>The Collective Communication Analyzer (CoMMA) is now generally available for optimizing large-scale AI and ML training workloads. It collects detailed NCCL telemetry for GPU machine types, helping to analyze performance and operational events. CoMMA is automatically installed on specific GPU machine types and can be managed through configuration or manual installation on others.</summary>
    <title>Feature: Collective Communication Analyzer (CoMMA) Now Generally Available</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-07-07:feature:0</id>
    <published>2025-07-07T00:00:00+00:00</published>
    <updated>2025-07-07T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Preview</strong>: You can use future reservations in calendar mode to obtain resources for up to 90 days. By creating a request in calendar mode, you can reserve up to 80 GPU VMs for a future date and time. Then, you can use that capacity to run the following workloads:</p>
<ul>
<li>Model pre-training</li>
<li>Model fine-tuning</li>
<li>Simulations</li>
<li>Inference</li>
</ul>
<p>For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/consumption-models">Choose a consumption option</a>.</p>]]></content>
    <summary>You can now reserve up to 80 GPU VMs for up to 90 days in advance using calendar mode. This feature is ideal for workloads such as model pre-training, fine-tuning, simulations, and inference.</summary>
    <title>Feature: Future Reservations Now Available</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:ai-hypercomputer:2025-06-11:feature:0</id>
    <published>2025-06-11T00:00:00+00:00</published>
    <updated>2025-06-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Generally available</strong>: You can apply a workload policy in a managed instance group (MIG) to specify the type of the workload to run on the MIG. Workload policies help improve the workload performance by optimizing the underlying infrastructure. The supported type, <code>high-throughput</code>, is ideal for workloads that require high networking performance. For more information, see <a href="https://cloud.google.com/ai-hypercomputer/docs/placement-policy-and-workload-policy#workload-policy">Workload policy for MIGs</a>.</p>]]></content>
    <summary>You can now apply a workload policy to a managed instance group (MIG) to optimize underlying infrastructure for better workload performance. The supported 'high-throughput' type is ideal for workloads needing high networking performance.</summary>
    <title>Feature: New Workload Policy Feature for MIGs</title>
  </entry>
</feed>