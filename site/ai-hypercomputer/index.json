{
  "name": "AI Hypercomputer",
  "entries": {
    "2025-12-12": [
      {
        "kind": "Issue",
        "content": "Workloads on A4 VMs might experience interruptions due to a firmware issue\nfor NVIDIA B200 GPUs. To help prevent the issue, we recommend resetting the GPUs\non A4 VMs at least once every 60 days. For more information, see the\n[known issue](https://docs.cloud.google.com/ai-hypercomputer/docs/troubleshooting/known-issues#a4-firmware).",
        "summary": {
          "title": "GPU Firmware Issue on A4 VMs",
          "summary": "A firmware issue with NVIDIA B200 GPUs may cause interruptions on A4 VMs. To mitigate this, reset your GPUs on A4 VMs at least once every 60 days."
        }
      }
    ],
    "2025-12-09": [
      {
        "kind": "Feature",
        "content": "**Generally available**: You can enable node health prediction in an\nAI-optimized GKE cluster to help the cluster avoid scheduling workloads on nodes\nthat are likely to degrade within the next five hours. This approach helps\nminimize interruptions for critical and interruption-sensitive workloads, such\nas large-scale training. For more information, see\n[Enable node health prediction in a GKE cluster](https://docs.cloud.google.com/ai-hypercomputer/docs/workloads/enable-node-health-prediction).",
        "summary": {
          "title": "Node health prediction for GKE",
          "summary": "AI-optimized GKE clusters now support node health prediction, a new feature that helps avoid scheduling workloads on nodes likely to degrade within five hours. This enhancement minimizes interruptions for critical and interruption-sensitive workloads like large-scale training."
        }
      }
    ],
    "2025-10-21": [
      {
        "kind": "Feature",
        "content": "**Generally available**: You can use future reservations in\nAI Hypercomputer to request to reserve capacity starting on a specific\ndate up to one year in the future. For more information, see\n[Reserve capacity](https://docs.cloud.google.com/ai-hypercomputer/docs/reserve-capacity).",
        "summary": {
          "title": "Future reservations are now available",
          "summary": "You can now reserve capacity in AI Hypercomputer starting on a specific date up to one year in advance. This feature allows for better planning and resource management."
        }
      }
    ],
    "2025-09-10": [
      {
        "kind": "Feature",
        "content": "**Generally available**: The [accelerator-optimized A4X machine type](https://cloud.google.com/ai-hypercomputer/docs/gpu#a4x), the first GPU VM to run on Arm, is available on AI Hypercomputer. The A4X machine series has the NVIDIA GB200 Grace Blackwell Superchips attached and runs on the NVIDIA GB200 NVL72 platform. Use this machine type to run your large artificial intelligence (AI) models and machine learning (ML) workloads. The A4X machine type is currently available in the `us-central1-a` zone.",
        "summary": {
          "title": "A4X machine type now generally available",
          "summary": "The new accelerator-optimized A4X machine type, featuring NVIDIA GB200 Grace Blackwell Superchips, is now generally available. This GPU VM, running on the NVIDIA GB200 NVL72 platform, is ideal for large AI and ML workloads and is currently available in the `us-central1-a` zone."
        }
      },
      {
        "kind": "Feature",
        "content": "**Generally available**: You can receive a notification when maintenance for an A4X reservation sub-block is scheduled, starts, or is completed. Additionally, you can now view and trigger maintenance for an A4X reservation sub-block. These features give you more control over maintenance for your A4X reservations, helping you minimize downtimes for your workloads. For more information, see [Manage host events across reservations](https://cloud.google.com/ai-hypercomputer/docs/manage/host-events-reservations).",
        "summary": {
          "title": "A4X reservation maintenance notifications and controls",
          "summary": "Stay informed and in control of your A4X reservation maintenance with new notifications and management features."
        }
      },
      {
        "kind": "Feature",
        "content": "You can receive at least seven days of advance notice for unplanned hardware maintenance for a reservation. This feature helps you more proactively control disruptions to your workloads when unplanned maintenance is scheduled after a host error or faulty host report. For more information, see [Manage hardware emergency maintenance notifications](https://cloud.google.com/ai-hypercomputer/docs/manage/host-events-reservations#emergency-notifications).",
        "summary": {
          "title": "Advance notice for hardware maintenance",
          "summary": "Receive at least seven days of advance notice for unplanned hardware maintenance for a reservation."
        }
      },
      {
        "kind": "Feature",
        "content": "**Generally available**: You can use the following Cloud Monitoring metrics to monitor your A4X VMs, and help you identify and troubleshoot issues with your GPUs:\n\n* NVLink runtime error\n* Uncorrectable DRAM ECC errors\n* Uncorrectable DRAM row remapping count\n* Uncorrectable DRAM row remapping failed\n* Uncorrectable PCIe errors\n* Uncorrectable cache ECC errors\n\nFor more information, see [Monitor VMs and Slurm clusters](https://cloud.google.com/ai-hypercomputer/docs/monitor).",
        "summary": {
          "title": "New metrics for A4X VM monitoring",
          "summary": "New metrics are available to monitor your A4X VMs and troubleshoot GPU issues."
        }
      },
      {
        "kind": "Feature",
        "content": "**Generally available**: You can view and manage the topology of your A4X reservations, including sub-blocks. This feature helps you better understand the topology of the VMs in your workload to further minimize network latency, as well as understand the health of your reservation blocks or sub-blocks. For more information, see\n[View reserved capacity](https://cloud.google.com/ai-hypercomputer/docs/view-reserved-capacity).",
        "summary": {
          "title": "View A4X reservation topology",
          "summary": "You can now view and manage the topology of your A4X reservations, including sub-blocks. This helps you understand VM topology for reduced network latency and monitor reservation health."
        }
      },
      {
        "kind": "Feature",
        "content": "**Generally available**: When you reserve capacity for creating VMs, you can specify the reservation operational mode for your reserved capacity. A reservation operational mode defines how your VMs behave after a host error or faulty host report, and it determines your level of visibility and control over the reservation's infrastructure. For more information, see [Reservation operational mode](https://cloud.google.com/ai-hypercomputer/docs/create/review-configurations#reservation-operational-mode).",
        "summary": {
          "title": "Reservation operational mode for VMs",
          "summary": "You can now specify the reservation operational mode for your VM capacity reservations. This setting determines how your VMs behave in case of host errors, offering different levels of control and visibility over the underlying infrastructure."
        }
      },
      {
        "kind": "Feature",
        "content": "**Generally available**: When you reserve capacity for creating VMs, you can specify a maintenance scheduling type for your reservations. This feature helps you minimize downtimes by letting you specify whether you want to group VMs and have synchronized maintenance scheduling (*grouped*), or loosely couple VMs have independent maintenance scheduling (*independent*). For more information, see [Maintenance scheduling types](https://cloud.google.com/ai-hypercomputer/docs/create/review-configurations#maintenance-scheduling-types).",
        "summary": {
          "title": "VMware Cloud Foundation on Google Cloud VMware Engine",
          "summary": "You can now specify maintenance scheduling types for your VM reservations to minimize downtime."
        }
      }
    ],
    "2025-07-18": [
      {
        "kind": "Feature",
        "content": "**Generally available**: You can troubleshoot workloads with slow performance by using straggler detection metrics and logs.\n\n*Stragglers* are single-point, non-crashing failures that eventually\nslow down your entire workload. Large-scale ML workloads are very susceptible to stragglers, and VMs with stragglers are often very difficult to notice and pinpoint without straggler detection.\n\nFor more information, see [Monitor VMs and Slurm clusters](https://cloud.google.com/ai-hypercomputer/docs/monitor) and [Troubleshoot slow performance](https://cloud.google.com/ai-hypercomputer/docs/troubleshooting/troubleshoot-slow-performance).",
        "summary": {
          "title": "New Feature: Straggler Detection for Performance Troubleshooting",
          "summary": "Troubleshoot slow performance in your workloads by using new straggler detection metrics and logs. This feature helps identify and resolve single-point, non-crashing failures that can slow down your entire workload, especially in large-scale ML workloads."
        }
      }
    ],
    "2025-07-10": [
      {
        "kind": "Feature",
        "content": "**Generally available**: You can now manage the [Collective Communication Analyzer (CoMMA)](https://cloud.google.com/ai-hypercomputer/docs/nccl/comma), a library that uses the NVIDIA Collective Communication Library (NCCL) profiler plugin to collect detailed NCCL telemetry for GPU machine types. The collected performance metrics and operational events are used for analyzing and optimizing large-scale AI and ML training workloads.\n\nCoMMA is automatically installed and enabled on A4X, A4 High, and A3 Ultra machine types when using specific images. You can manage this data collection by disabling the plugin, adjusting its data granularity levels, or manually installing it on other GPU machine types. For more information, see [Enable, disable, and configure CoMMA](https://cloud.google.com/ai-hypercomputer/docs/nccl/configure-comma).",
        "summary": {
          "title": "Collective Communication Analyzer (CoMMA) Now Generally Available",
          "summary": "The Collective Communication Analyzer (CoMMA) is now generally available for optimizing large-scale AI and ML training workloads. It collects detailed NCCL telemetry for GPU machine types, helping to analyze performance and operational events. CoMMA is automatically installed on specific GPU machine types and can be managed through configuration or manual installation on others."
        }
      }
    ],
    "2025-07-07": [
      {
        "kind": "Feature",
        "content": "**Preview**: You can use future reservations in calendar mode to obtain resources for up to 90 days. By creating a request in calendar mode, you can reserve up to 80 GPU VMs for a future date and time. Then, you can use that capacity to run the following workloads:\n\n* Model pre-training\n* Model fine-tuning\n* Simulations\n* Inference\n\nFor more information, see [Choose a consumption option](https://cloud.google.com/ai-hypercomputer/docs/consumption-models).",
        "summary": {
          "title": "Future Reservations Now Available",
          "summary": "You can now reserve up to 80 GPU VMs for up to 90 days in advance using calendar mode. This feature is ideal for workloads such as model pre-training, fine-tuning, simulations, and inference."
        }
      }
    ],
    "2025-06-11": [
      {
        "kind": "Feature",
        "content": "**Generally available**: You can apply a workload policy in a managed instance group (MIG) to specify the type of the workload to run on the MIG. Workload policies help improve the workload performance by optimizing the underlying infrastructure. The supported type, `high-throughput`, is ideal for workloads that require high networking performance. For more information, see [Workload policy for MIGs](https://cloud.google.com/ai-hypercomputer/docs/placement-policy-and-workload-policy#workload-policy).",
        "summary": {
          "title": "New Workload Policy Feature for MIGs",
          "summary": "You can now apply a workload policy to a managed instance group (MIG) to optimize underlying infrastructure for better workload performance. The supported 'high-throughput' type is ideal for workloads needing high networking performance."
        }
      }
    ]
  }
}