<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Google Cloud: Dataflow</title>
  <subtitle>Changelog for Google Cloud product: Dataflow</subtitle>
  <updated>2025-09-24T00:00:00+00:00</updated>
  <id>urn:github:multani:gcp-changelog:dataflow</id>
  <link rel="self" href="https://raw.githubusercontent.com/multani/gcp-changelog/refs/heads/main/content/dataflow/"/>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-09-24:feature:0</id>
    <published>2025-09-24T00:00:00+00:00</published>
    <updated>2025-09-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For jobs that use GPUs, Dataflow now supports the flex-start provisioning model. This flex-start provisioning model can improve your ability to get access to constrained GPU resources for short-duration workloads. This feature is available in Preview and is for batch pipelines only. For more information, see <a href="https://cloud.google.com/dataflow/docs/gpu/use-gpus#optional_configure_a_provisioning_model">Configure a provisioning model</a>.</p>]]></content>
    <summary>Dataflow now supports flex-start provisioning for GPU jobs, improving access to scarce GPU resources for short-term workloads. This feature is in Preview and intended for batch pipelines.</summary>
    <title>Feature: Dataflow GPU Access</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-09-08:feature:0</id>
    <published>2025-09-08T00:00:00+00:00</published>
    <updated>2025-09-08T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow now supports using <a href="https://cloud.google.com/firewall/docs/tags-firewalls-overview">secure tags</a> to set firewall rules on worker VMs. For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/routes-firewall#secure_tags">Use secure tags with Dataflow</a>.</p>]]></content>
    <summary>Dataflow now supports secure tags for firewall rules on worker VMs.</summary>
    <title>Feature: Secure Tags for Dataflow Workers</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-27:feature:0</id>
    <published>2025-08-27T00:00:00+00:00</published>
    <updated>2025-08-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow supports TPUs, Google's custom-designed AI accelerators that are optimized for large-scale AI/ML workloads. This feature lets you accelerate inference workloads on frameworks like PyTorch, JAX, and TensorFlow. This feature is <a href="https://cloud.google.com/products#product-launch-stages">generally available</a> with an allowlist. For more information, see <a href="https://cloud.google.com/dataflow/docs/tpu/tpu-support">Dataflow support for TPUs</a>.</p>]]></content>
    <summary>Accelerate your AI/ML inference workloads with TPUs on Dataflow. Generally available with an allowlist.</summary>
    <title>Feature: Dataflow TPU Support</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-27:feature:1</id>
    <published>2025-08-27T00:00:00+00:00</published>
    <updated>2025-08-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow supports <a href="https://cloud.google.com/compute/docs/instances/reservations-overview#consumption-type"><em>specifically targeted</em> reservations</a> for pipelines using accelerators (GPUs or TPUs). This functionality is generally available with an allowlist. For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/compute-engine-reservations.md#reservations-accelerators">Use Compute Engine reservations with Dataflow</a>.</p>]]></content>
    <summary>Dataflow now generally available supports [*specifically targeted* reservations](https://cloud.google.com/compute/docs/instances/reservations-overview#consumption_type) for pipelines using accelerators (GPUs or TPUs). For more information, see [Use Compute Engine reservations with Dataflow](https://cloud.google.com/dataflow/docs/guides/compute-engine-reservations.md#reservations-accelerators).</summary>
    <title>Feature: Dataflow now supports reservations for pipelines with accelerators.</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-27:changed:2</id>
    <published>2025-08-27T00:00:00+00:00</published>
    <updated>2025-08-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow supports NVIDIA® H100 and NVIDIA® H100 Mega GPU types. For more information, see <a href="https://cloud.google.com/dataflow/docs/gpu/gpu-support">Dataflow support for GPUs</a>.</p>]]></content>
    <summary>Dataflow now supports NVIDIA H100 and H100 Mega GPUs, enhancing processing capabilities. For detailed information, refer to the official documentation on GPU support.</summary>
    <title>Changed: NVIDIA GPU Support in Dataflow</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-26:fixed:0</id>
    <published>2025-08-26T00:00:00+00:00</published>
    <updated>2025-08-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow <a href="https://cloud.google.com/dataflow/docs/runner-v2">Runner v2</a> fixes an issue that could cause data discrepancies when using splittable DoFns, particularly when processing large datasets as side inputs. This fix ensures that all data is accurately processed and transmitted within the pipeline. This improvement is available in recent Dataflow service releases, and is automatically enabled when using Dataflow Runner v2.</p>
<p><strong>Note:</strong> After this fix, pipelines that previously experienced data loss due to this issue might consume more resources (such as CPU, memory, and processing time) because more data is being processed. This increase in resource usage is expected and reflects the correct behavior of the pipeline.</p>]]></content>
    <summary>Runner v2 resolves data discrepancies with splittable DoFns and large side inputs. Data is now processed and transmitted accurately.</summary>
    <title>Fixed: Dataflow Runner v2 Fixes Data Discrepancies</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-11:feature:0</id>
    <published>2025-08-11T00:00:00+00:00</published>
    <updated>2025-08-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow now automatically detects performance bottlenecks in streaming jobs. You can see the cause of the bottleneck in the <strong>Step Info</strong> panel to help with troubleshooting.</p>
<p>For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/troubleshoot-bottlenecks">Troubleshoot bottlenecks</a>.</p>]]></content>
    <summary>Dataflow now automatically detects performance bottlenecks in streaming jobs.</summary>
    <title>Feature: Dataflow Bottleneck Detection</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-06-26:feature:0</id>
    <published>2025-06-26T00:00:00+00:00</published>
    <updated>2025-06-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow now supports an automated parallel update workflow for streaming jobs. This feature helps minimize disruption by launching a new replacement job that runs in parallel with the existing job. After a duration of time you specify, the old job is automatically drained.</p>
<p>For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/upgrade-guide#run-parallel-pipelines">Run parallel pipelines</a>.</p>]]></content>
    <summary>Dataflow now supports an automated parallel update workflow for streaming jobs. This feature helps minimize disruption by launching a new replacement job that runs in parallel with the existing job. After a duration of time you specify, the old job is automatically drained.</summary>
    <title>Feature: Automated parallel update workflow for streaming jobs</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-06-09:feature:0</id>
    <published>2025-06-09T00:00:00+00:00</published>
    <updated>2025-06-09T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow now supports right fitting for streaming jobs. <em>Right fitting</em> lets you specify resource requirements for an entire pipeline or for specific pipeline steps. Previously, right fitting was only supported for batch pipelines. For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/right-fitting#streaming-right-fitting">Streaming right fitting</a>.</p>]]></content>
    <summary>Dataflow now supports right fitting for streaming jobs, allowing you to specify resource requirements for your entire pipeline or specific steps. This feature was previously only available for batch pipelines.</summary>
    <title>Feature: Dataflow now supports right fitting for streaming jobs</title>
  </entry>
</feed>