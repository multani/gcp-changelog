<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Google Cloud: Dataflow</title>
  <subtitle>Changelog for Google Cloud product: Dataflow</subtitle>
  <updated>2025-08-27T00:00:00+00:00</updated>
  <id>urn:github:multani:gcp-changelog:dataflow</id>
  <link rel="self" href="https://raw.githubusercontent.com/multani/gcp-changelog/refs/heads/main/content/dataflow/"/>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-27:feature:0</id>
    <published>2025-08-27T00:00:00+00:00</published>
    <updated>2025-08-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow supports Cloud TPUs, Google's custom-designed AI accelerators that are optimized for large-scale AI/ML workloads. This feature lets you accelerate inference workloads on frameworks like PyTorch, JAX, and TensorFlow. This feature is <a href="https://cloud.google.com/products#product-launch-stages">generally available</a> with an allowlist. For more information, see <a href="https://cloud.google.com/dataflow/docs/tpu/tpu-support">Dataflow support for TPUs</a>.</p>]]></content>
    <summary>Dataflow now supports Cloud TPUs for accelerated AI/ML workloads.</summary>
    <title>Feature: Dataflow TPU Support</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-26:fixed:0</id>
    <published>2025-08-26T00:00:00+00:00</published>
    <updated>2025-08-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow <a href="https://cloud.google.com/dataflow/docs/runner-v2">Runner v2</a> fixes an issue that could cause data discrepancies when using splittable DoFns, particularly when processing large datasets as side inputs. This fix ensures that all data is accurately processed and transmitted within the pipeline. This improvement is available in recent Dataflow service releases, and is automatically enabled when using Dataflow Runner v2.</p>
<p><strong>Note:</strong> After this fix, pipelines that previously experienced data loss due to this issue might consume more resources (such as CPU, memory, and processing time) because more data is being processed. This increase in resource usage is expected and reflects the correct behavior of the pipeline.</p>]]></content>
    <summary>Runner v2 resolves data discrepancies with splittable DoFns and large side inputs. Data is now processed and transmitted accurately.</summary>
    <title>Fixed: Dataflow Runner v2 Fixes Data Discrepancies</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-08-11:feature:0</id>
    <published>2025-08-11T00:00:00+00:00</published>
    <updated>2025-08-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow now automatically detects performance bottlenecks in streaming jobs. You can see the cause of the bottleneck in the <strong>Step Info</strong> panel to help with troubleshooting.</p>
<p>For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/troubleshoot-bottlenecks">Troubleshoot bottlenecks</a>.</p>]]></content>
    <summary>Dataflow now automatically detects performance bottlenecks in streaming jobs.</summary>
    <title>Feature: Dataflow Bottleneck Detection</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-06-26:feature:0</id>
    <published>2025-06-26T00:00:00+00:00</published>
    <updated>2025-06-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow now supports an automated parallel update workflow for streaming jobs. This feature helps minimize disruption by launching a new replacement job that runs in parallel with the existing job. After a duration of time you specify, the old job is automatically drained.</p>
<p>For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/upgrade-guide#run-parallel-pipelines">Run parallel pipelines</a>.</p>]]></content>
    <summary>Dataflow now supports an automated parallel update workflow for streaming jobs. This feature helps minimize disruption by launching a new replacement job that runs in parallel with the existing job. After a duration of time you specify, the old job is automatically drained.</summary>
    <title>Feature: Automated parallel update workflow for streaming jobs</title>
  </entry>
  <entry>
    <author>
      <name>Google Cloud</name>
    </author>
    <id>urn:github:multani:gcp-changelog:dataflow:2025-06-09:feature:0</id>
    <published>2025-06-09T00:00:00+00:00</published>
    <updated>2025-06-09T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Dataflow now supports right fitting for streaming jobs. <em>Right fitting</em> lets you specify resource requirements for an entire pipeline or for specific pipeline steps. Previously, right fitting was only supported for batch pipelines. For more information, see <a href="https://cloud.google.com/dataflow/docs/guides/right-fitting#streaming-right-fitting">Streaming right fitting</a>.</p>]]></content>
    <summary>Dataflow now supports right fitting for streaming jobs, allowing you to specify resource requirements for your entire pipeline or specific steps. This feature was previously only available for batch pipelines.</summary>
    <title>Feature: Dataflow now supports right fitting for streaming jobs</title>
  </entry>
</feed>